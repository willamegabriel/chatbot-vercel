import { HuggingFaceTransformersEmbeddings } from "@langchain/community/embeddings/hf";
import { FAISS } from "@langchain/community/vectorstores/faiss";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { RetrievalQAChain } from "langchain/chains";
import path from "path";

export const config = {
  runtime: "nodejs"
};

export default async function handler(req, res) {
  console.log("ğŸ“¡ RequisiÃ§Ã£o recebida:", req.method);

  if (req.method !== "POST") {
    return res.status(405).json({ error: "MÃ©todo nÃ£o permitido" });
  }

  const { pergunta } = req.body;
  console.log("â“ Pergunta recebida:", pergunta);

  if (!pergunta || typeof pergunta !== "string") {
    return res.status(400).json({ error: "Pergunta ausente ou invÃ¡lida" });
  }

  try {
    console.log("ğŸ§  Iniciando embeddings...");
    const embeddings = new HuggingFaceTransformersEmbeddings({
      modelName: "sentence-transformers/all-MiniLM-L6-v2"
    });

    const basePath = path.resolve(process.cwd(), "vetores_site");
    console.log("ğŸ“ Carregando FAISS de:", basePath);
    const vectorstore = await FAISS.load(basePath, embeddings);
    console.log("âœ… Vetores carregados");

    const model = new ChatOpenAI({
      temperature: 0.2,
      modelName: "gpt-3.5-turbo",
      openAIApiKey: process.env.OPENAI_API_KEY
    });

    console.log("ğŸ§© Criando chain de QA...");
    const chain = RetrievalQAChain.fromLLM(model, vectorstore.asRetriever());

    console.log("ğŸš€ Executando chain com a pergunta...");
    const resposta = await chain.run(pergunta);

    console.log("âœ… Resposta gerada com sucesso");
    return res.status(200).json({ resposta });
  } catch (erro) {
    console.error("âŒ Erro ao gerar resposta:", erro);
    return res.status(500).json({ error: "Erro interno ao processar a pergunta" });
  }
}
